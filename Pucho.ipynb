{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys \n",
    "#import glob\n",
    "import re\n",
    "import nltk\n",
    "from collections import Counter\n",
    "nltk.download()  # Download text data sets, including stop words\n",
    "from nltk.corpus import stopwords # Import the stop word list\n",
    "from nltk.stem.porter import *\n",
    "from bs4 import BeautifulSoup \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "traindf=pd.read_csv('/home/harshit/Documents/PUCHO/labeledTrainData.tsv',sep='\\t')\n",
    "#unlabeledtraindf=pd.read_csv('/home/harshit/Documents/PUCHO/unlabeledTrainData.tsv', header=0, \n",
    " #delimiter=\"\\t\", quoting=3 )\n",
    "testdf=pd.read_csv('/home/harshit/Documents/PUCHO/testData.tsv',sep='\\t')\n",
    "#unlabeledtraindf.head()\n",
    "#traindf.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#traindf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_cleaner( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    return( \" \".join( meaningful_words ))\n",
    "    #return(sequence.pad_sequences(sentences, maxlen=1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/bs4/__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "# Get the number of reviews based on the dataframe column size\n",
    "num_reviews = traindf[\"review\"].size\n",
    "\n",
    "num_testreviews=testdf[\"review\"].size\n",
    "# Initialize an empty list to hold the clean reviews\n",
    "clean_train_reviews = []\n",
    "clean_test_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in xrange( 0, num_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_train_reviews.append( review_cleaner( traindf[\"review\"][i] ) )\n",
    "for j in xrange( 0, num_testreviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    clean_test_reviews.append( review_cleaner( testdf[\"review\"][j] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(clean_train_reviews[3])\n",
    "#testdf[\"review\"][4]\n",
    "#clean_test_reviews[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordlist_for_word2vec( raw_review ):\n",
    "    # Function to convert a raw review to a string of words\n",
    "    # The input is a single string (a raw movie review), and \n",
    "    # the output is a single string (a preprocessed movie review)\n",
    "    #\n",
    "    # 1. Remove HTML\n",
    "    #count=0\n",
    "    review_text = BeautifulSoup(raw_review).get_text() \n",
    "    #\n",
    "    # 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split() \n",
    "#     count+=len(words)\n",
    "#     print(count)\n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    #\n",
    "    #stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    #m_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join(words ))  \n",
    "    #return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the punkt tokenizer\n",
    "# tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# def review_to_sentences( review, tokenizer):#, remove_stopwords=False ):\n",
    "#     # Function to split a review into parsed sentences. Returns a \n",
    "#     # list of sentences, where each sentence is a list of words\n",
    "#     #\n",
    "#     # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "#     raw_sentences = tokenizer.tokenize(review.strip().decode(\"utf8\"))\n",
    "#     #\n",
    "#     # 2. Loop over each sentence\n",
    "#     bsentences = []\n",
    "#     for raw_sentence in raw_sentences:\n",
    "#         # If a sentence is empty, skip it\n",
    "#         if len(raw_sentence) > 0:\n",
    "#             bsentences.append( review_to_wordlist_for_word2vec( raw_sentence))\n",
    "#     return bsentences\n",
    "                             \n",
    "    \n",
    "#     # Return the list of sentences (each sentence is a list of words,\n",
    "#     # so this returns a list of lists\n",
    "                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# sentences = []  # Initialize an empty list of sentences\n",
    "# count=0\n",
    "# print \"Parsing sentences from training set\"\n",
    "# for review in traindf[\"review\"]:\n",
    "#     count=count+1\n",
    "#     sentences += review_to_sentences(review, tokenizer)\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'but with mark evading the law they realize that he needs to keep running'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentences[25002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print \"Parsing sentences from unlabeled set\"\n",
    "# count=0\n",
    "# for review in unlabeledtraindf[\"review\"]:\n",
    "#     count=count+1\n",
    "#     sentences += review_to_sentences(review, tokenizer)\n",
    "# print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from test set\n"
     ]
    }
   ],
   "source": [
    "# test_sentences=[]\n",
    "# count=0\n",
    "# print \"Parsing sentences from test set\"\n",
    "# for review in testdf[\"review\"]:\n",
    "#     count=count+1\n",
    "#     test_sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(test_sentences[1:3])\n",
    "#print count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/keras/preprocessing/text.py:157: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 74065 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(nb_words=15000)\n",
    "tokenizer.fit_on_texts(clean_train_reviews)\n",
    "sequences = tokenizer.texts_to_sequences(clean_train_reviews)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tokenizer.fit_on_texts(clean_test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_sequences = tokenizer.texts_to_sequences(clean_test_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(sequences[6])\n",
    "data = pad_sequences(sequences, maxlen=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data=pad_sequences(test_sequences, maxlen=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(data[27227])\n",
    "#word_index['fawn']\n",
    "#(word_index) \n",
    "#(sentences[1]) \n",
    "#len(sequences)\n",
    "#sequences[1:3]\n",
    "#test_data[3]\n",
    "#sequences[3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Import the built-in logging module and configure it so that Word2Vec \n",
    "# # creates nice output messages\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "#     level=logging.INFO)\n",
    "\n",
    "# # Set values for various parameters\n",
    "# num_features = 300    # Word vector dimensionality                      \n",
    "# min_word_count = 30   # Minimum word count                        \n",
    "# num_workers = 4       # Number of threads to run in parallel\n",
    "# context = 10          # Context window size                                                                                    \n",
    "# #negative=10\n",
    "# downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# # Initialize and train the model (this will take some time)\n",
    "# from gensim.models import word2vec\n",
    "# print \"Training model...\"\n",
    "# model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "#             size=num_features, min_count = min_word_count, \\\n",
    "#             window = context, sample = downsampling)\n",
    "\n",
    "# # If you don't plan to train the model any further, calling \n",
    "# # init_sims will make the model much more memory-efficient.\n",
    "# model.init_sims(replace=True)\n",
    "\n",
    "# # It can be helpful to create a meaningful model name and \n",
    "# # save the model for later use. You can load it later using Word2Vec.load()\n",
    "# model_name = \"300features_40minwords_10context\"\n",
    "# model.wv.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-02-03 11:41:53,076 : INFO : loading EuclideanKeyedVectors object from 300features_40minwords_10context\n",
      "2018-02-03 11:41:53,654 : INFO : setting ignored attribute syn0norm to None\n",
      "2018-02-03 11:41:53,658 : INFO : loaded 300features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "# Load the model that we created in Part 2\n",
    "# from gensim.models import Word2Vec\n",
    "# model=KeyedVectors.load(model_name)\n",
    "#model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "# print(type(model.wv.syn0))\n",
    "# (model.wv.syn0)#.shape)\n",
    "#(model.wv.index2word)\n",
    "# model.wv.syn0[1]\n",
    "#model.wv.syn0['this']\n",
    "#model['this']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# index_dict={}\n",
    "# for index,word in enumerate(model.wv.index2word):\n",
    "#     index_dict[word] = index+1\n",
    "\n",
    "\n",
    "#     #index_dict={word:index+1}\n",
    "# #index_dict['this']\n",
    "# #sorted(index_dict)\n",
    "# #index_dict.items()\n",
    "# len(index_dict)\n",
    "# #model['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# word_vectors={}\n",
    "# for word,index in index_dict.items():\n",
    "#     word_vectors[word]=model[word]\n",
    "# #len(word_vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model[\"movie\"]\n",
    "# Summarize review length of each sentence using X[i]\n",
    "# print(\"Review length of traindata: \")\n",
    "# result = [len(x) for x in traindf['review']]\n",
    "# print(\"Mean %.2f words (%f)\" % (np.mean(result), np.std(result)))\n",
    "# print(\"Review length of unlabeledtraindf: \")\n",
    "# result2 = [len(x) for x in unlabeledtraindf['review']]\n",
    "# print(\"Mean %.2f words (%f)\" % (np.mean(result2), np.std(result2)))\n",
    "# print(\"Review length of testdata: \")\n",
    "# result3 = [len(x) for x in testdf['review']]\n",
    "# print(\"Mean %.2f words (%f)\" % (np.mean(result3), np.std(result3)))\n",
    "#std to check for distribution\n",
    "#mean to calculate the average length of a review\n",
    "\n",
    "\n",
    "# Review length of traindata: \n",
    "# Mean 1327.95 words (1005.530261)\n",
    "# Review length of unlabeledtraindf: \n",
    "# Mean 1332.82 words (1007.186870)\n",
    "# Review length of testdata: \n",
    "# Mean 1296.68 words (978.384307)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def makeFeatureVec(words, model, num_features):\n",
    "#     # Function to average all of the word vectors in a given\n",
    "#     # paragraph\n",
    "#     #\n",
    "#     # Pre-initialize an empty numpy array (for speed)\n",
    "#     featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "#     #\n",
    "#     nwords = 0\n",
    "#     # \n",
    "#     # Index2word is a list that contains the names of the words in \n",
    "#     # the model's vocabulary. Convert it to a set, for speed \n",
    "#     index2word_set = set(model.wv.index2word)\n",
    "#     #\n",
    "#     # Loop over each word in the review and, if it is in the model's\n",
    "#     # vocaublary, add its feature vector to the total\n",
    "#     for word in words:\n",
    "#         if word in index2word_set: \n",
    "#             nwords = nwords + 1.\n",
    "#             featureVec = np.add(featureVec,model[word])\n",
    "#     # \n",
    "#     # Divide the result by the number of words to get the average\n",
    "#     featureVec = np.floor_divide(featureVec,nwords)\n",
    "    \n",
    "#     return featureVec\n",
    "\n",
    "\n",
    "# def getAvgFeatureVecs(reviews, model, num_features):\n",
    "#     # Given a set of reviews (each one a list of words), calculate \n",
    "#     # the average feature vector for each one and return a 2D numpy array \n",
    "#     # \n",
    "#     # Initialize a counter\n",
    "#     counter = 0\n",
    "#     # \n",
    "#     # Preallocate a 2D numpy array, for speed\n",
    "#     reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "#     # \n",
    "#     # Loop through the reviews\n",
    "#     for review in reviews:\n",
    "#        #\n",
    "#        # Print a status message every 1000th review\n",
    "#         if counter%1000 == 0:\n",
    "#             print \"Review %d of %d\" % (counter, len(reviews))\n",
    "#        # \n",
    "#        # Call the function (defined above) that makes average feature vectors\n",
    "#         reviewFeatureVecs[counter] = makeFeatureVec(review, model,num_features)\n",
    "#        #\n",
    "#        # Increment the counter\n",
    "#         counter = (counter) + 1\n",
    "#     return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# trainDataVecs = getAvgFeatureVecs( clean_train_reviews, model, num_features )\n",
    "# testDataVecs = getAvgFeatureVecs( clean_test_reviews, model, num_features )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74065"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # assemble the embedding_weights in one numpy array\n",
    "# vocab_dim = 7 # dimensionality of your word vectors\n",
    "# n_symbols = len(word_index) + 1 # adding 1 to account for 0th index (for masking)\n",
    "# embedding_weights = np.zeros((n_symbols, vocab_dim))\n",
    "# for word,index in word_index.items():\n",
    "#     embedding_weights[index, :] = data[index]\n",
    "\n",
    "# # embedding_layer = Embedding(len(word_index) + 1,\n",
    "# #                             EMBEDDING_DIM,\n",
    "# #                             weights=[embedding_matrix],\n",
    "# #                             input_length=MAX_SEQUENCE_LENGTH,\n",
    "# #                             trainable=False)\n",
    "# # define inputs here\n",
    "# embedding_layer = Embedding(output_dim=vocab_dim, input_dim=n_symbols, trainable=True)\n",
    "# embedding_layer.build((None,)) # if you don't do this, the next step won't work\n",
    "# embedding_layer.set_weights([embedding_weights])\n",
    "\n",
    "# #embedded = embedding_layer(input_layer)\n",
    "# # ... continue model definition here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_layer=Embedding(input_dim=len(word_index)+1, output_dim=64,input_length=1500)\n",
    "modelk = Sequential()\n",
    "modelk.add(embedding_layer)\n",
    "modelk.add(LSTM(128))\n",
    "modelk.add(Dropout(0.5))\n",
    "modelk.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "modelk.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# modelk.fit(x_train, y_train, batch_size=16, epochs=10)\n",
    "# score = modelk.evaluate(x_test, y_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# modelk = Sequential() # a layer model\n",
    "# modelk.add(embedding_layer)\n",
    "# #model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "# modelk.add(Flatten())\n",
    "# modelk.add(Dense(250, activation='relu'))\n",
    "# modelk.add(Dense(1, activation='sigmoid'))\n",
    "# modelk.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# print(modelk.summary())\n",
    "#len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = data[:]\n",
    "X_test=test_data[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0, ...,   23, 1090,   33], dtype=int32)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      " - 1853s - loss: 0.3902 - acc: 0.8334\n",
      "Epoch 2/8\n",
      " - 1839s - loss: 0.2387 - acc: 0.9097\n",
      "Epoch 3/8\n",
      " - 1876s - loss: 0.2031 - acc: 0.9262\n",
      "Epoch 4/8\n",
      " - 1871s - loss: 0.1782 - acc: 0.9356\n",
      "Epoch 5/8\n",
      " - 1858s - loss: 0.1608 - acc: 0.9425\n",
      "Epoch 6/8\n",
      " - 1851s - loss: 0.1444 - acc: 0.9495\n",
      "Epoch 7/8\n",
      " - 1859s - loss: 0.1255 - acc: 0.9572\n",
      "Epoch 8/8\n",
      " - 1878s - loss: 0.1069 - acc: 0.9644\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd74233bd90>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "modelk.fit(X_train, traindf[\"sentiment\"], epochs=8,  verbose=2)\n",
    "#modelk.fit(X,y, epochs=2,  verbose=2)\n",
    "# Final evaluation of the model\n",
    "#result = modelk.predict(testDataVecs, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test & extract results \n",
    "result = modelk.predict(X_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4990441 ],\n",
       "       [0.49804902],\n",
       "       [0.50155693],\n",
       "       ...,\n",
       "       [0.5004143 ],\n",
       "       [0.5019524 ],\n",
       "       [0.5013185 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# result.shape\n",
    "# testdf['id'].shape\n",
    "# testdf['id'].type\n",
    "# result.type\n",
    "fresult=result.tolist()\n",
    "ids=testdf['id'].tolist()\n",
    "#len(fresult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#(fresult.type())\n",
    "#\", \".join(fresult)\n",
    "#fresult[0].type\n",
    "flattened_list = [y for x in fresult for y in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#len(flattened_list)\n",
    "binary_result = [round(x,0) for x in flattened_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i,value in enumerate(fresult):\n",
    "#     if value>0.486:\n",
    "#         fresult[i]=1\n",
    "#     else:\n",
    "#         fresult[i]=0\n",
    "# (binary_result)\n",
    "solution=[int(i) for i in binary_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "d = {'id': ids, 'sentiment': solution}\n",
    "output = pd.DataFrame(data=d)\n",
    "# Write the test results \n",
    "#output = pd.DataFrame( data={\"id\":testdf[\"id\"], \"sentiment\":result} )\n",
    "output.to_csv( \"Word2Vec_Keras_AverageVectors.csv\", index=False, quoting=3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "independent_dataset=pd.read_csv('/home/harshit/Documents/PUCHO/indDataset.tsv',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_ind_reviews=independent_dataset[\"review\"].size\n",
    "\n",
    "ind_reviews = []\n",
    "\n",
    "# Loop over each review; create an index i that goes from 0 to the length\n",
    "# of the movie review list \n",
    "for i in xrange( 0, num_ind_reviews ):\n",
    "    # Call our function for each one, and add the result to the list of\n",
    "    # clean reviews\n",
    "    ind_reviews.append( review_cleaner( independent_dataset[\"review\"][i] ) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_sequences = tokenizer.texts_to_sequences(ind_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_data=pad_sequences(ind_sequences, maxlen=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ind_result=modelk.predict(ind_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ind_result\n",
    "ind_result_list=ind_result.tolist()\n",
    "ids=independent_dataset['id'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "flattened_ilist = [y for x in ind_result_list for y in x]\n",
    "binary_iresult = [round(x) for x in flattened_ilist]\n",
    "isolution=[int(i) for i in binary_iresult]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_true=independent_dataset['sentiment']\n",
    "# modelk.binary_accuracy(y_true,isolution)\n",
    "isolution"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
